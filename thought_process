----------------------------------------------
----------------------------------------------
Tuesday, Sept 10th:
----------------------------------------------
----------------------------------------------

Let's explore the following few questions today:

(a): Is the current implementation of FASH correct? Specifically, is the marginal likelihood correctly computed?

Answer: The function `simulate_nonlinear_function` assumes 50 basis of o-spline, whereas the implementation uses 30. This might change the result slightly.
Also, there should not be a linear trend added for the case when p = 1, but the function `simulate_nonlinear_function` always add it...
I have modified the function, and let's try the experiment again to see the result.
Actually that result works well now!


(b): Can we estimate the mixture proportion $\pi$ accurately if the number of observation or the number of measurement is large? How much does the result depend on the standard deviation of the noise?

Answer: If the number of measurement is 16, and we have N = 1000, we can already estimate well the pi_0 and pi_1 for the case of two mixtures.
It also did quite well when we increase the number of mixtures to five.
If we increase the search space of the mixture components (meaning more pi_i to be estimated), the result suprisingly is still not bad.
If we increase the standard deviation of the noise, of course it significantly impairs the estimation of the mixture proportion.


(c): How much can pooling information across datasets help with the estimation of the psd?

Answer: Let's see through an experiment: first we estimate the psd for each dataset through ML.
For both the high noise (sd = 1) and the low noise (sd = 0), the EB method improves the estimation of the psd compared to the ML method.
The improvement is largest for the high noise case.
What if we have a larger number of datasets? (N = 1000 -> N = 3000) Well, there are more datasets to share information with, I believe it will further improve the performance of EB.
Let's see the result: Suprisingly, the result does not become significantly better for EB at the largest two values of the psd. My intuition is that these two groups have too small proportions, so their results are kind of pulling down by the other groups with larger proportions, and could not receive as much beneifts from the pooling information.
We can check again when we increase their proportions: wow.. it actually makes the difference between EB and MLE smaller.. I think the reason is that the two largest psd are at the boundary of the search space, so the EB method could not estimate them well.
What if we increase the limit of the grid that we are searching (so the two largest psd are no longer at the boundary of the search space): The EB approach is almost always better at different true psd values, only except the largest psd value when SD is very high.


----------------------------------------------
----------------------------------------------
Wed, Sept 11th:
----------------------------------------------
----------------------------------------------

Today, let's stick to the sanity check example from yesterday, and
try to see how the EB method performs in terms of FDR and Power when
testing dynamic eQTLs (i.e. when psd > 0).

(a): How does the FDR of FASH compare to the FDR of MASH?

Answer: Both methods performed very well in terms of FDR control in this simple setting.
When betaprec is specified to be very diffuse, I don't think the conclusion will change for FASH.
Let's see through a sensitivity experiment if we specify betaprec to be 0.00001: indeed, the
result does not change.

(b): How does the Power of FASH compare to the Power of MASH?

Answer: FASH has slightly better power when FDR is between 0 to 0.5.
When betaprec is specified to be very diffuse, I don't think the conclusion will change for FASH.
Let's see through a sensitivity experiment if we specify betaprec to be 0.00001: indeed, the
result does not change.


(c) Answer the same two questions, when the comparison is against the oracle Bayes method.

Answer: The EB (FASH) and OB (oracle) have extremely similar results! Except OB has better estimation accuracy at the largest value of the true PSD.



----------------------------------------------
----------------------------------------------
Thu, Sept 12th:
----------------------------------------------
----------------------------------------------

Today, I am revisiting the previous examples, with the updated cpp code for the Gaussian likelihood computation at the base model (aka sigma = 0).

(a): For the Simulation 1 (Pollutant) example, what would happen with this code change?

Answer: Not any significant change, except that the PSD that was estimated to be zero now becomes some non-zero small value. 
No more prior weight is assigned to the zero case now.

(b): For the original Simulation 2 (testing eQTL), what would happen?

Answer: First, the power of the FASH approach is significantly boosted, and it seems like the inference no longer being sensitive to the choice of beta_prec.
However, the estimation of pi_0 seems to be consistently lower than the true, causing the FDR calibration to be non-accurate. 
I am investigating if this is something wrong in my code or this is something for real...
This problem does not occur when the model is correct... (see the new version of Simulation 2 when there is no model-misspecification).
Would this problem still occur if we only compare linear vs non-dynamic eQTLs? ---> Still underestimate by the same amount (0.65 where the truth is 0.7).
So if we remove those linear observations? ---> still not correct... now it is estimated as 0.63...
What if we change the dynamic-simulation so it draws from the IWP? There should be no more model-misspecification then... --> Indeed, it fixes the problem.
So we conclude, for the FASH approach to have nice FDR calibration. Model-specification is indeed important!
This kind of make sense too.. Even ASH underestimates \pi_0 when the true model is bi-modal. Here we can think of the model being bi-modal too: one mode around the
Constant space, and a smaller mode around some other space (for example, linear space).

(c): Since the original FASH is sensitive to model-misspecification (so is ash, so nothing extremely surprising though), how can we make it more robust?

Answer: Idea 1 -> How about we introduce a penalty that is just like the one from ash?
Without using the correction (pi_0 = 0.5, whereas pi_0 estimate is 0.43).
Now let's try to incorporate that penalty into FASH... We can do that by adding \lambda_0-1 number of observations with likelihood being 1 at the null
Component and 0 at the other components. It is important to use the original (not log) likelihood matrix then.
It improves pi_0 from 0.43 to 0.45 in this particular example.
If we reduce the number of linear cases, and increase pi to 0.7, try again we get --> (unpenalized result 0.6685535), (penalized result 0.6753572).
It indeed helps a little, using the default lambda_0 = 10 suggested in ash.
If we increase lambda_0 to 50, it helps to make our estimate of pi_0 conservative in FASH.
Finally, let's try again when using a more refined grid of sigma in the mixture --> Not much difference!





----------------------------------------------
----------------------------------------------
Fri, Sept 13th:
----------------------------------------------
----------------------------------------------

Today, we will continue to revisit some examples of FASH using the corrected code of the likelihood computation.


(a): For the expression data analysis, will the result change if we increase the number of knots to 16, and make the PSD equally placed between 0 to 1?

Answer: The result indeed changes. Many genes are estimated to have larger deviation from the linearity now.


(b): For the COVID-19 example, will the likelihood computation change any result?

Answer: The result basically does not change.


(c): For detecting periodicity: can we try (i): mixture of sGP, and then test for sigma = 0, if beta_s is significantly nonzero?
					or (ii): mixture of sGP and mixture of IWP? Then test sGP vs IWP?

Answer: My feeling is both approaches could be used. For approach (i): directly testing sigma = 0 should give the FDR on detecting 
Exact periodicity. Further including the FSR based on the joint distribution of \beta_sin and \beta_cos can make the inference more
Robust.
For approach (ii): the question becomes detecting periodic or quasi-periodic behavior...
Let's focus on the second approach for now. The important lesson, to make the result invariant to diffuse beta_prec, each model should have the same 
Number of diffuse terms.





----------------------------------------------
----------------------------------------------
Mon, Sept 16th:
----------------------------------------------
----------------------------------------------

There are two things that I would like to explore today. Both of them are for the circadian detection simulation:

(A): For the periodicity detection setting, when there are no model misspecification (aka. The null model is from IWP2, and the alternative model is from sGP), will there be any FDR calibration issue?

Answer: The estimated prior proportions are still not accurate.. and the FDR is still not calibrated.
Maybe the problem is due to the approximation error of Laplace approximation? Or is it just some coding issue..
Let me check again once we increase the number of counts in each x value. This should reduce the approximation error of the Laplace approximation to the marginal likelihood.
The result indeed seems to be slightly better when the number of counts increases. Let's increase the number of measurement from 50 to 100 to check its behaviour as well. Indeed, the result is further improved a little.
Another check, if the approximation error is the problem, then it should also affect the oracle Bayes procedure. 
Let's check the oracle Bayes: Indeed.. Even the oracle result has similar calibration issue...
If we significantly increase the count numbers, can we reduce the approximation effect to practically zero?
Indeed, the FASH approach then has result very close to the true FDR curve. The oracle Bayes is closer to the truth, almost indistinguishable.
So the issue is indeed the Laplace approximation accuracy when the count number is low. Both methods have satisfactory FDR calibration when the number of count is high.
Wait!!! I just notice that there was an issue with the posterior matrix computation! The prior was mis-treated as the log-prior!! Let's update and retry it.
Wow!! The result is perfect now!!!! (Also needs to expand the grid of the PSD too) 

(B): When there are model-misspecification, can we consider more robust multiple hypothesis controlling method than FDR? How about max-lfdr?

Answer: Actually even for the model-misspecification case, if we increase the number of count to reduce the Laplace approximation error, the FDR becomes quite well calibrated. 
To double check this, let me rerun the misspecified case with ten times of the datasets (N1 = 1000 and N2 = 2000 now). The calibration of FASH's FDR does not change by much.
If we still care about the non-preciseness of the calibration, how about we increase the possibility of the null-hypothesis (considering a wider range of psd for IWP) --> This helps a lot too!!


(C): What would be the main underlying effect of model-misspecification in the FDR control setting? How does it affect the power and FDR calibration of FASH?

Answer: My intuition is: when the alternative hypothesis is wrongly specified (see eQTL example, where the alternative hypothesis should be linear function, but wrongly specified as Brownian motion), the corresponding likelihood components tend to be under-estimated, which leads to larger estimate of pi_0 hence lower power.
When the null hypothesis is wrongly specified (for example, the non-quasi-periodic behaviour is random B spline basis, rather than samples of IWP2), the likelihood of the null component will be under estimated, leading to lower estimate of pi_0 hence under-conservative FDR.
To check if this is true, let's consider the following experiment:
(i): If the alternative hypothesis (quasi-periodicity) is simulated from the sGP. But the null hypothesis is not simulated from the IWP2.
(ii): Vice versa, if the null is correctly simulated from IWP2, but the alternative hypothesis is not from sGP.
I expect to see (ii) to cause lower power and (i) to cause incorrect calibration. 
Let's try (i) first: For the circadian detection example, when alternative is mis-specified, pi_0 is over-estimated, leading to lower power.
Then try (ii): For the circadian detection example, mis-specification of the null hypothesis under-estimates pi_0 leading to incorrect (inflated) FDR calibration.
Note that although this phenomenon happens most often, this is not an absolute result. We could come up with example where significant mis-specification of the alternative model could lead to under-estimate of pi_0. 
For example, if the null is constant, the true alternative is linear function, but the fitted alternative is IWP1. 
Introducing those linear functions may lead to underestimation of pi_0. Why? Let's explore this tomorrow! 





----------------------------------------------
----------------------------------------------
Tue, Sept 17th:
----------------------------------------------
----------------------------------------------

Today, I will continue on the exploration of my question from yesterday: why in the eQTL example, the misspecification of the alternative hypothesis (true: linear, fitted: IWP1) leads to under-estimation of pi_0 hence inflated FDR control?

(a): Hypothesis 1: Linear terms with larger slopes are causing the issue. Because their larger slopes mislead the likelihood, they are significantly more likely to be in the IWP groups than the null group. So they penalize the value of pi_0 even more severely than regular observations from the IWP groups.

Answer: To check if this is the reason, we can truncate the normal distribution used to generate the random slopes, and see if the result gets better.
If I truncate the (abs value) normal slopes at 1, the result still does not improve by much.
Trying truncating at 0.5 just to double check -> the result still does not change by much.
So it is unlikely caused by the abs size of the random slope...

(b): Hypothesis 2: All linear terms could affect the estimation of pi_0. As the distances to the null space happen to be even larger than the distance for a fitted true alternative from the null space.

Answer: Let's take a look at the likelihood matrix for the true alternative and the false alternative (linear terms) ---> No it does not look like this case. The likelihood seems to be less informative than those from the true alternative.

(c): Let's further simplify the context, consider equal standard deviation, and assuming the slope is fixed at beta = 0.5. See if it changes anything.

Answer: Based on the controlled experiment, it seems like the main factor that affects the performance is the value of the noise in the dataset. 
When the datasets are quite noisy (for example sd = 1 or 5), the pi_0 could be severely underestimated.
But when the dataset has small noise (for example sd = 0.1 or 0.3), the estimate of pi_0 is fairly accurate.
Let's pick the case when sd = 0.1, and see if the result will change once we allow the slope to be random --> still works fairly well...

(d): Here is one possible explanation based on the experiment result today: When the noise level is high (high standard deviation), the likelihoods for observations from the constant or the IWP group are very flat. However, the likelihoods for the observations from the linear group with larger slope are not flat. So they dominate the estimation of pi_0 and break the equilibrium created by those correct flat likelihoods...

Answer: If this is the reason, then we should see their likelihood vectors differ in structure to the likelihood vectors of the other two groups.
Let's check how does the structure of the likelihood varies as the noise level changes, for each of the three groups. 
When sigma = 1, many linear models penalize the null case significant with very small likelihood. Many of the linear models penalizes the null case more severely than the many of the true alternative.
If we decrease sigma to 0.3, 





