---
title: "simulation_eQTL"
author: "Ziang Zhang"
date: "2024-05-19"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## **Context**

We consider the effect size estimate of eQTLs measured in day $t = 1$ to day $t = 16$.
Among the $x$ dynamic eQTLs, we assume $x$ eQTLs have linear dynamic and the other $x$ eQTLs have non-linear dynamic.
Among the $x$ eQTLs with non-linear dynamic, we assume $x$ of them have strong non-linearity and the other $x$ have weak non-linearity.

For simplicity, let's assume the effect size estimate of eQTL $i$ at time $j$ is constant: $\text{SE}(\hat{\beta}_{i}(t_j)) = \sigma_i, \forall j$ and hence $\hat{\beta}_i(t_j) \sim N(\beta_i(t_j),\sigma^2)$.

```{r}
library(BayesGP)
library(TMB)
library(Matrix)
library(splines)
library(parallel)
library(ggplot2)
library(reshape2)
library(mixsqp)
library(tidyverse)
library(mashr)
cpp_dir <- paste0(getwd(), "/code/cpp")
fig_dir <- paste0(getwd(), "/output/simulation_eQTL")
result_dir <- paste0(getwd(), "/output/simulation_eQTL")
function_dir <- paste0(getwd(), "/code/function")
source(paste0(function_dir, "/functions_fitting_Gaussian_eQTL.R"))
source(paste0(function_dir, "/functions_simulation_eQTL.R"))
compile(paste0(cpp_dir, "/Gaussian_theta_known.cpp"))
compile(paste0(cpp_dir, "/Gaussian_just_fixed.cpp"))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_theta_known")))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_just_fixed")))
num_cores <- 4
set.seed(123)
each_size <- 500
sigma <- sample(c(0.05, 0.1, 0.3), size = (3*each_size), replace = TRUE, prob = c(1/3,1/3,1/3))
```

Simulate the data for the linear dynamic eQTLs.

```{r}
# set.seed(1)
# data_sim <- simulate_process(sd = sigma[1], sd_fun = 0.1, type = "linear")
# plot(data_sim$x, data_sim$y, 
#      ylim = c(-3,3),
#      type = "o", col = "blue", 
#      lwd = 2, xlab = "Time", ylab = "estimated effect")
# lines(data_sim$x, data_sim$truef, col = "red", lwd = 2)

# Repeat for each_size times
n_sim <- each_size
data_sim_list_A <- lapply(1:n_sim, function(i) simulate_process(sd = sigma[i], sd_linear = 0.1, type = "linear"))
```


Simulate the data for the non-linear dynamic eQTLs.

```{r}
# Repeat for each_size times with weak non-linearity
n_sim <- each_size
data_sim_list_B <- lapply(1:n_sim, function(i) simulate_process(n_basis = 3, sd = sigma[i + (each_size)], sd_fun = 1, sd_linear = 0.1, type = "nonlinear"))

# Repeat for each_size times with medium non-linearity
n_sim <- each_size
data_sim_list_C <- lapply(1:n_sim, function(i) simulate_process(n_basis = 8, sd = sigma[i + (2*each_size)], sd_fun = 1, sd_linear = 0.1, type = "nonlinear"))
```


For each group of eQTLs, let's visualize four examples
```{r}
par(mfrow=c(2,2))
for (i in 1:4) {
  plot(data_sim_list_A[[i]]$x, data_sim_list_A[[i]]$y, 
       type = "o", col = "blue", 
       lwd = 2, xlab = "Time", ylab = "estimated effect")
  lines(data_sim_list_A[[i]]$x, data_sim_list_A[[i]]$truef, col = "red", lwd = 2)
  title(paste0("Type A: Linear Dynamic eQTL ", i))
}
```

```{r}
par(mfrow=c(2,2))
for (i in 1:4) {
  plot(data_sim_list_B[[i]]$x, data_sim_list_B[[i]]$y, 
       type = "o", col = "blue", 
       lwd = 2, xlab = "Time", ylab = "estimated effect")
  lines(data_sim_list_B[[i]]$x, data_sim_list_B[[i]]$truef, col = "red", lwd = 2)
  title(paste0("Type B: Non-linear Dynamic eQTL ", i))
}
```

```{r}
par(mfrow=c(2,2))
for (i in 1:4) {
  plot(data_sim_list_C[[i]]$x, data_sim_list_C[[i]]$y, 
       type = "o", col = "blue", 
       lwd = 2, xlab = "Time", ylab = "estimated effect")
  lines(data_sim_list_C[[i]]$x, data_sim_list_C[[i]]$truef, col = "red", lwd = 2)
  title(paste0("Type C: Non-Linear dynamic eQTL ", i))
}
par(mfrow=c(1,1))
```


## **Model Fitting with FASH**

### **Empirical Bayes**

Combine all the data list into a single list of length 
```{r}
datasets <- c(data_sim_list_A, data_sim_list_B, data_sim_list_C)
```

```{r, eval=FALSE}
set.seed(123)
p_vec <- 2
# log_prec <- unique(sort(c(Inf, seq(-1,1, by = 0.1), seq(1,5, by = 0.5), seq(5,10, by = 1)), decreasing = T))
# psd_iwp_vec <- 1/exp(.5*log_prec)
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
L_vecs <- list()
# create a progress bar
pb <- txtProgressBar(min = 0, max = length(datasets), style = 3)
for (i in 1:length(datasets)) {
  setTxtProgressBar(pb, i)
  L_vecs[[i]] <- compute_log_likelihood_ospline_seq2(
    x = datasets[[i]]$x,
    y = datasets[[i]]$y,
    p = p_vec,
    num_knots = 16,
    psd_iwp_vector = psd_iwp_vec,
    pred_step = 1,
    betaprec = 0.001,
    sd_gaussian = sigma[i]
  )
}
L_matrix <- do.call(rbind, L_vecs)
save(L_matrix, file = paste0(result_dir, "/L_matrix.rda"))
```

```{r, echo=FALSE}
set.seed(123)
p_vec <- 2
# log_prec <- unique(sort(c(Inf, seq(-1,1, by = 0.1), seq(1,5, by = 0.5), seq(5,10, by = 1)), decreasing = T))
# psd_iwp_vec <- 1/exp(.5*log_prec)
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
load(paste0(result_dir, "/L_matrix.rda"))
```


Then, we make use of $\texttt{mixsqp}$ to estimate the prior weights:  
```{r}
fit.sqp <- mixsqp(L = L_matrix, log = TRUE)
numiter <- nrow(fit.sqp$progress)
plot(1:numiter,fit.sqp$progress$objective,type = "b",
     pch = 20,lwd = 2,xlab = "SQP iteration",
     ylab = "objective",xaxp = c(1,numiter,numiter - 1))
prior_weight <- data.frame(p = rep(p_vec, each = length(psd_iwp_vec)), psd_iwp = psd_iwp_vec, prior_weight = fit.sqp$x)
```

We can take a look at the estimated prior:
```{r}
head(prior_weight)
```

### **Posterior Inference**

With the estimated prior, we can now perform the posterior inference for each dataset:

```{r, eval=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
posterior_weights_matrix <- matrix(nrow = num_datasets, ncol = num_weights)

# Loop through each dataset and perform fitting
fitted_datasets <- list()
# start a progress bar
pb <- txtProgressBar(min = 0, max = num_datasets, style = 3)
for (i in seq_along(datasets)) {
  setTxtProgressBar(pb, i)
  dataset <- datasets[[i]]
  fit_result_final <- fit_ospline_with_prior2(
    num_cores = 6,
    x = dataset$x,
    y = dataset$y,
    num_knots = 16,
    prior_weight = prior_weight,
    betaprec = 0.001,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets[[i]] <- aggregate_fit_with_prior(x = dataset$x, fit_results_with_prior = fit_result_final)$summary_df
  posterior_weights_matrix[i, ] <- fit_result_final$posterior_weights[, "posterior_weight"]
}
colnames(posterior_weights_matrix) <- paste(as.character(fit_result_final$posterior_weights[, "p"]),
                                            as.character(fit_result_final$posterior_weights[, "psd_iwp"]), sep = "_")
save(posterior_weights_matrix, file = paste0(result_dir, "/posterior_weights_matrix.rda"))
save(fitted_datasets, file = paste0(result_dir, "/fitted_datasets.rda"))
```

```{r, echo=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
load(paste0(result_dir, "/posterior_weights_matrix.rda"))
load(paste0(result_dir, "/fitted_datasets.rda"))
```


We can visualize the posterior weights for each dataset:
```{r}
posterior_weights_df <- as.data.frame(posterior_weights_matrix)
posterior_weights_df$id <- 1:nrow(posterior_weights_df)
melted_data <- melt(posterior_weights_df, id.vars = "id")
melted_data$variable2 <- sub("_.*", "", melted_data$variable)
# melted_data$variable3 <- as.factor(round(as.numeric(sub("*._", "", melted_data$variable)), 3))
melted_data$variable3 <- (round(as.numeric(sub("*._", "", melted_data$variable)), 3))

ggplot(melted_data, aes(x = id, y = value, fill = variable3)) +
  geom_bar(stat = "identity") +
  labs(x = "Observation ID", y = "Weight", fill = "PSD") +
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "blue") +
  ggtitle("Structure Plot of Posterior Weights") +
  coord_flip() 
```

## **Effect Estimation**

Compare the posterior mean of the effect with single smoothing based on maximizing L-matrix; with MASH; with single smoothing using smoothing spline.

### **Comparison in terms of RMSE**

```{r}
fitted_datasets_ss <- list()
for (i in seq_along(datasets)) {
  dataset <- datasets[[i]]
  ss_fit <- smooth.spline(x = dataset$x, y = dataset$y)
  fitted_datasets_ss[[i]] <- data.frame(x = dataset$x, mean = predict(ss_fit, dataset$x)$y)
}
```

```{r, eval=FALSE}
fitted_datasets_ml <- list()
for (i in seq_along(datasets)) {
  dataset <- datasets[[i]]
  L_vec_selected <- L_matrix[i, ]
  which_sigma_max <- psd_iwp_vec[which.max(L_vec_selected)]
  fit_ospline_single <- fit_ospline(
    x = dataset$x,
    y = dataset$y,
    p = 2,
    num_knots = 16,
    psd_iwp = which_sigma_max,
    betaprec = 0.001,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets_ml[[i]] <- data.frame(x = dataset$x, mean = apply(fit_ospline_single$samps_fitted, 1, mean))
}
save(fitted_datasets_ml, file = paste0(result_dir, "/fitted_datasets_ml.rda"))
```

```{r, echo=FALSE}
load(paste0(result_dir, "/fitted_datasets_ml.rda"))
```

```{r}
fitted_datasets_mash <- list()

# Produce a huge data-matrix, the i-th row being dataset[[i]]$y
all_data_matrix <- do.call(rbind, lapply(datasets, function(x) x$y))
SE_matrix <- matrix(nrow = nrow(all_data_matrix), ncol = ncol(all_data_matrix), sigma)

# now use mashr:
mash_data <-  mashr::mash_set_data(all_data_matrix, SE_matrix)
m.1by1 = mashr::mash_1by1(mash_data)
strong = mashr::get_significant_results(m.1by1,1)[1:each_size]
U.pca = mashr::cov_pca(mash_data, 5, subset = strong)
U.ed = cov_ed(mash_data, U.pca, subset=strong)
U.c = cov_canonical(mash_data)  
m   = mash(mash_data, c(U.c,U.ed))
mash.pm <- get_pm(m)

# each row of mash.pm is the posterior mean of the effect size
for (i in 1:nrow(mash.pm)) {
  fitted_datasets_mash[[i]] <- data.frame(x = datasets[[i]]$x, mean = mash.pm[i, ])
}
```

```{r}
rmse_df <- data.frame(FASH = numeric(length = num_datasets), 
                      MASH = numeric(length = num_datasets), 
                      SS = numeric(length = num_datasets), 
                      ML = numeric(length = num_datasets),
                      RAW = numeric(length = num_datasets))

for (i in 1:nrow(rmse_df)) {
  rmse_df$FASH[i] <- sqrt(mean((fitted_datasets[[i]]$mean - datasets[[i]]$truef)^2))
  rmse_df$MASH[i] <- sqrt(mean((fitted_datasets_mash[[i]]$mean - datasets[[i]]$truef)^2))
  rmse_df$ML[i] <- sqrt(mean((fitted_datasets_ml[[i]]$mean - datasets[[i]]$truef)^2))
  rmse_df$RAW[i] <- sqrt(mean((datasets[[i]]$y - datasets[[i]]$truef)^2))
  rmse_df$SS[i] <- sqrt(mean((fitted_datasets_ss[[i]]$mean - datasets[[i]]$truef)^2))
}

rmse_df_relative <- rmse_df/rmse_df$RAW

ggplot(melt(log(rmse_df_relative)), aes(x = variable, y = value, fill = variable)) +
  geom_violin(draw_quantiles = c(0.25, 0.5, 0.75)) +
  labs(x = "Method", y = "rMSE") +
  theme_minimal() +
  ggtitle("Comparison of log rMSE between FASH, MASH, SS and ML")

ggplot(melt((rmse_df_relative)), aes(x = variable, y = value, fill = variable)) +
  geom_boxplot() +
  labs(x = "Method", y = "rMSE") +
  theme_minimal() +
  ggtitle("Comparison of rMSE between FASH, MASH, SS and ML")

```

FASH does not perform much better than ML, which is not surprise since each dataset has the same number of measurement and same level of SE, so pooling information does not help too much.

What if each observation series has missing observations at random and with different level of SE?

```{r}
rmse_df_compare <- rmse_df
# convert rmse to relative to RAW
rmse_df_compare <- rmse_df_compare / rmse_df$RAW

rmse_df_compare$sigma <- as.factor(sigma)
# ggplot of boxplot colored by method, facet by sigma, make ylim flexible
ggplot(melt((rmse_df_compare)), aes(x = variable, y = value, fill = variable)) +
  geom_boxplot() +
  coord_cartesian(ylim = c(0,1)) +
  labs(x = "Method", y = "rMSE") +
  theme_minimal() +
  facet_wrap(~sigma) +
  ggtitle("Comparison of rMSE between FASH, MASH, SS and ML")

rmse_df_compare <- rmse_df/ rmse_df$RAW
rmse_df_compare$type <- as.factor(rep(c("A", "B", "C"), each = each_size))
# ggplot of boxplot colored by method, facet by sigma
ggplot(melt(rmse_df_compare), aes(x = variable, y = value, fill = variable)) +
  geom_boxplot() +
  labs(x = "Method", y = "rMSE") +
  theme_minimal() +
  coord_cartesian(ylim = c(0,1)) + 
  facet_wrap(~type) +
  ggtitle("Comparison of rMSE between FASH, MASH, SS and ML")
```





## **Hypothesis Testing**

Do hypothesis testing using lfdr/lfsr control.

### **Which eQTLs are non-dynamic?**

The eQTLs with non-dynamic effect should have a smoothing parameter $\sigma = 0$ and slope of zero: $\beta(t) = \beta(t-1)$ for any $t$.

So, first compute $P(\sigma = 0|Data)$ and then compute $P(fs\text{ of }slope|\sigma=0,Data)$ and multiply to get $P(\sigma=0,fs\text{ of }slope|Data)$.


### **Which eQTLs have non-linear effect?**

Just compute $lfdr = P(\sigma = 0|Data)$.

### **Classification of dynamic eQTLs into early, late and switch**

Need to compute the posterior distribution of each required quantity.

#### *Early eQTLs*

#### *Late eQTLs*

#### *Switch eQTLs*

#### *Comparison with MASH*

