---
title: "eQTL Simulation"
author: "Ziang Zhang"
date: "2024-09-09"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## **Introduction**

We consider the effect size estimate of $10,000$ eQTLs measured in day $t = 1$ to day $t = 16$:

- there are $6000$ eQTLs that are not dynamic, i.e., the effect size is constant over time.
- there are $3000$ eQTLs that are dynamic, i.e., the effect size is changing over time.

Among the $3000$ dynamic eQTLs, we simulate their effect sizes from the oracle model.

For simplicity, let's assume the SE of effect estimate is constant over time: $\text{SE}(\hat{\beta}_{i}(t_j)) = \sigma_i, \forall j$ and hence: $$\hat{\beta}_i(t_j) \sim N(\beta_i(t_j),\sigma_i^2).$$

## **Data**

Load the required libraries and set the seed.
```{r, include = FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(BayesGP)
library(TMB)
library(Matrix)
library(splines)
library(parallel)
library(ggplot2)
library(reshape2)
library(mixsqp)
library(tidyverse)
library(mashr)
cpp_dir <- paste0(getwd(), "/code/cpp")
function_dir <- paste0(getwd(), "/code/function")
result_dir <- paste0(getwd(), "/output/eQTL_sim")

source(paste0(function_dir, "/functions_Gaussian_fitting_eQTLsim.R"))
source(paste0(function_dir, "/functions_simulating.R"))
compile(paste0(cpp_dir, "/Gaussian_theta_known.cpp"))
compile(paste0(cpp_dir, "/Gaussian_just_fixed.cpp"))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_theta_known")))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_just_fixed")))

num_cores <- 4
set.seed(123)
N <- 3000
sigma <- sample(c(0.1, 1), size = N, replace = TRUE, prob = c(1/2,1/2))
set.seed(123)
```

Let's simulate N independent datasets, each with different true effect curve:

```{r}
prop_vec <- c(0.6, 0.1, 0.1, 0.075, 0.075, 0.05)
psd_vec <- c(0.0, 0.1, 0.2, 0.3, 0.4, 0.5)
size_each <- c(N * prop_vec)
psd_sampled <- rep(psd_vec, size_each)

data_sim_list <- lapply(1:N, function(i) simulate_process(sd_poly = 1, type = "nonlinear", sd = sigma[i], psd_function = psd_sampled[i]))
par(mfrow = c(3, 3))
for (i in 1:9) {
  index <- sample(1:N, 1)
  plot(data_sim_list[[index]]$x, data_sim_list[[index]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list[[index]]$truef, data_sim_list[[index]]$y),
       ylab = "Effect size", main = paste0("eQTL ", index))
  points(data_sim_list[[index]]$x, data_sim_list[[index]]$y, col = "blue")
}
par(mfrow = c(1, 1))

sigma <- unlist(lapply(data_sim_list, function(x) unique(x$sd)))
```


## **Implementing FASH **

First, we compute the L matrix:

```{r eval=FALSE}
beta_prec <- 0.00001
set.seed(123)
p_vec <- 1
psd_iwp_vec <- sort(unique(c(0,seq(0, 1, by = 0.1))))
L_vecs <- list()
# create a progress bar
pb <- txtProgressBar(min = 0, max = length(data_sim_list), style = 3)
for (i in 1:length(data_sim_list)) {
  setTxtProgressBar(pb, i)
  L_vecs[[i]] <- compute_log_likelihood_ospline_seq2(
    x = data_sim_list[[i]]$x,
    y = data_sim_list[[i]]$y,
    p = p_vec,
    num_knots = 50,
    psd_iwp_vector = psd_iwp_vec,
    pred_step = 1,
    betaprec = beta_prec,
    sd_gaussian = sigma[i]
  )
}
L_matrix <- do.call(rbind, L_vecs)
save(L_matrix, file = paste0(result_dir, "/L_matrix.rda"))
```

```{r}
beta_prec <- 0.00001
set.seed(123)
p_vec <- 1
psd_iwp_vec <- sort(unique(c(0,seq(0, 1, by = 0.1))))
load(paste0(result_dir, "/L_matrix.rda"))
```


Based on the L-matrix, we optimize the prior weights through EB:

```{r}
fit.sqp <- mixsqp(L = L_matrix, log = TRUE)
numiter <- nrow(fit.sqp$progress)
plot(1:numiter,fit.sqp$progress$objective,type = "b",
     pch = 20,lwd = 2,xlab = "SQP iteration",
     ylab = "objective",xaxp = c(1,numiter,numiter - 1))
prior_weight <- data.frame(p = rep(p_vec, each = length(psd_iwp_vec)), psd_iwp = psd_iwp_vec, prior_weight = fit.sqp$x)
```

```{r}
head(prior_weight)
```


### *How does pooling help the estimation of $\sigma$?*

First, take out the ML estimate of the PSD $(\sigma)$ for each dataset:
```{r}
MLE_psd <- L_matrix %>% apply(MARGIN = 1, function(x) psd_iwp_vec[which.max(x)]) %>% unlist()
PSD_data <- data.frame(truth = psd_sampled, MLE = MLE_psd)
```

How much is the MSE?
```{r}
PSD_data$sd <- sigma
mean((MLE_psd - psd_sampled)^2)
```

Now compute the posterior mean using FASH:
```{r eval=FALSE}
num_datasets <- length(data_sim_list)
num_weights <- sum(prior_weight$prior_weight != 0)
posterior_weights_matrix <- matrix(nrow = num_datasets, ncol = num_weights)

# Loop through each dataset and perform fitting
fitted_datasets <- list()

pb <- txtProgressBar(min = 0, max = N, style = 3)
for (i in seq_along(data_sim_list)) {
  setTxtProgressBar(pb, i)
  dataset <- data_sim_list[[i]]
  fit_result_final <- fit_ospline_with_prior2(
    num_cores = 1,
    x = dataset$x,
    y = dataset$y,
    num_knots = 50,
    prior_weight = prior_weight,
    betaprec = beta_prec,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets[[i]] <- aggregate_fit_with_prior(x = dataset$x, fit_results_with_prior = fit_result_final)$summary_df
  posterior_weights_matrix[i, ] <- fit_result_final$posterior_weights[, "posterior_weight"]
}
colnames(posterior_weights_matrix) <- paste(as.character(fit_result_final$posterior_weights[, "p"]),
                                            as.character(fit_result_final$posterior_weights[, "psd_iwp"]), sep = "_")
save(posterior_weights_matrix, file = paste0(result_dir, "/posterior_weights_matrix.rda"))
```

```{r}
num_datasets <- length(data_sim_list)
num_weights <- sum(prior_weight$prior_weight != 0)
load(paste0(result_dir, "/posterior_weights_matrix.rda"))
```

```{r}
# compute the posterior mean
psd_iwp_vec_effective <- psd_iwp_vec[prior_weight$prior_weight != 0]
posterior_mean <- rowSums(posterior_weights_matrix * matrix(psd_iwp_vec_effective, nrow = num_datasets, ncol = length(psd_iwp_vec_effective), byrow = T))
PSD_data$EB <- posterior_mean
```

Compute its MSE:

```{r}
mean((posterior_mean - psd_sampled)^2)
```


Compare the two plots for MLE and EB (FASH):

```{r}
# Combine MLE and EB data into a single dataframe
PSD_data_long <- PSD_data %>%
  pivot_longer(cols = c("MLE", "EB"), names_to = "Method", values_to = "Estimate")

# Calculate the Mean Squared Error (MSE) for each method and SD
plot_combined <- PSD_data_long %>%
  group_by(truth, sd, Method) %>%
  summarize(MSE = mean((Estimate - truth)^2)) %>%
  ggplot(aes(x = truth, y = MSE, color = Method)) + 
  geom_point() + 
  geom_line() +
  facet_wrap(~ sd, scales = "free_y") +  # Facet by SD
  ggtitle("Comparison of MLE and EB MSE across SD values") + 
  labs(x = "Truth", y = "MSE", color = "Method") +
  ylim(0, 0.15) +
  theme_minimal()

# Print the combined plot
print(plot_combined)
```


Also compare the boxplot of the squared error for MLE and EB:

```{r}
PSD_data$error_EB <- (PSD_data$EB - PSD_data$truth)^2
PSD_data$error_MLE <- (PSD_data$MLE - PSD_data$truth)^2

# Reshape the data for ggplot
PSD_data_melted <- melt(PSD_data, id.vars = "sd", measure.vars = c("error_EB", "error_MLE"), variable.name = "Method", value.name = "Error")

# Calculate mean and standard deviation for each group
summary_data <- PSD_data_melted %>%
  group_by(sd, Method) %>%
  summarise(
    mean_error = mean(Error),
    sd_error = sd(Error),
    .groups = 'drop'
  )

# Create the error bar plot with mean as a dot, ensuring ymin is at least 0
ggplot(summary_data, aes(x = as.factor(sd), y = mean_error, color = Method)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = pmax(mean_error - 2 * sd_error, 0), ymax = mean_error + 2 * sd_error), 
                position = position_dodge(width = 0.5), width = 0.2) +
  labs(title = "Comparison of Errors in EB and MLE Methods with Error Bars (Â±2 SD)", 
       x = "SD", y = "Mean Error") +
  theme_minimal() +
  scale_color_discrete(name = "Method")
```

This illustrates that by pooling the information through EB, the posterior mean from FASH improves the estimate of $\sigma$ compared to the original MLE.


### *Implementing FDR control through FASH*

Here we utilize the false discovery rate provided by FASH to test the dynamic eQTLs.

Compute the local false discovery rate (lfdr):
```{r}
set.seed(123)
lfdr <- posterior_weights_matrix[,1]
fdr_df <- data.frame(eQTL = 1:length(lfdr), fdr = lfdr, type = rep(c("ND","D"), times = c(size_each[1],N-size_each[1])))
fdr_df <- fdr_df[order(fdr_df$fdr), ] # ordering it
fdr_df$cumulative_fdr <- cumsum(fdr_df$fdr)/seq_along(fdr_df$fdr)
fdr_df$rank <- 1:length(lfdr)
```

Plot the FDR curve:

```{r}
ggplot(fdr_df, aes(x = 1:length(lfdr), y = cumulative_fdr, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDR", col = "Type") +
  theme_minimal() +
  ggtitle("FASH:Cumulative FDR Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```

How many false discoveries are there?

```{r}
alpha <- 0.05
num_discoveries <- sum(fdr_df$cumulative_fdr <= alpha)
num_false_discoveries <- sum(fdr_df$cumulative_fdr <= alpha & fdr_df$type == "ND")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
# 0.04878049
```

Plot the curve of nominal false discovery rate (threshold) against the actual false discovery rate:

```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "ND")
  fdr_vec[i] <- num_false_discoveries / num_discoveries
}

# Create a data frame for plotting
fdr_df_fash_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec)

# Plot the nominal FDR vs true FDR for FASH
ggplot(fdr_df_fash_for_plotting, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "(Nominal) False Discovery Rate", y = "(Actual) False Discovery Rate") +
  theme_minimal() +
  geom_hline(yintercept = size_each[1]/N, linetype = "dashed", color = "red") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs Actual FDR Curve for FASH")
```


Also compare the number of false discoveries with the number of discoveries at different level of threshold, where number of discoveries is plotted against number of false discoveries:

```{r}
threshold_vec <- seq(0, 1, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "ND")
}
num_discoveries_df <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df, aes(x = (num_discoveries), y = (num_false_discoveries))) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("FASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, size_each[1]), ylim = c(0, size_each[1]))
```




## **Comparison with MASH**

Now, we compare the performance of FASH and MASH, in testing dynamic eQTLs. Let's do the comparison with the default implementation of MASH:

```{r, eval=FALSE}
fitted_datasets_mash <- list()

# Produce a huge data-matrix, the i-th row being dataset[[i]]$y
all_data_matrix <- do.call(rbind, lapply(data_sim_list, function(x) x$y))
SE_matrix <- matrix(nrow = nrow(all_data_matrix), ncol = ncol(all_data_matrix), sigma)

# now use mashr:
mash_data <-  mashr::mash_set_data(all_data_matrix, SE_matrix)
m.1by1 = mashr::mash_1by1(mash_data)
strong = mashr::get_significant_results(m.1by1, 0.05)
# keep the top 10%
strong <- strong[1:round(0.1*length(strong))]
U.pca = mashr::cov_pca(mash_data, 5, subset = strong)
U.ed = cov_ed(mash_data, U.pca, subset=strong)
U.c = cov_canonical(mash_data)  
m   = mash(mash_data, c(U.c,U.ed))
save(m, file = paste0(result_dir, "/mash_result.rda"))
```

```{r}
load(paste0(result_dir, "/mash_result.rda"))
mash_post <- m$posterior_weights
## extract the colnames start with "equal_effects"
lfdr_mash <- mash_post[, c(1, grep("equal_effects", colnames(mash_post)))]
# sum each row
lfdr_mash <- rowSums(lfdr_mash)

fdr_df_mash <- data.frame(eQTL = 1:length(lfdr), fdr = lfdr_mash, type = rep(c("ND", "D"), times = c(size_each[1], N - size_each[1])))
fdr_df_mash <- fdr_df_mash[order(fdr_df_mash$fdr), ] # ordering it
fdr_df_mash$cumulative_fdr <- cumsum(fdr_df_mash$fdr)/seq_along(fdr_df_mash$fdr)
fdr_df_mash$rank <- 1:length(lfdr_mash)
```

How many false discoveries are there?

```{r}
alpha <- 0.05
num_discoveries <- sum(fdr_df_mash$cumulative_fdr <= alpha)
num_false_discoveries <- sum(fdr_df_mash$cumulative_fdr <= alpha & fdr_df_mash$type == "ND")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
```

Plot the FDR curve for MASH:

```{r}
ggplot(fdr_df_mash, aes(x = rank, y = cumulative_fdr, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDR", col = "Type") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 1)) +
  ggtitle("Cumulative FDR Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```


Check the calibration of FDR for MASH:

```{r}
# Calculate true FDR for MASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec_mash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "ND")
  fdr_vec_mash[i] <- num_false_discoveries_mash / num_discoveries_mash
}

# Create a data frame for plotting
fdr_df_mash_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_mash)

# Plot the nominal FDR vs true FDR for MASH
ggplot(fdr_df_mash_for_plotting, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "(Nominal) False Discovery Rate", y = "(Actual) False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs Actual FDR Curve for MASH")
```


Plot the number of false discoveries against the number of total discoveries for MASH:

```{r}
threshold_vec <- seq(0, 1, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "ND")
}
num_discoveries_df_mash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df_mash, aes(x = num_discoveries, y = (num_false_discoveries))) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("MASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, size_each[1]), ylim = c(0, size_each[1]))
```


Compare MASH (fdr_df_mash) and FASH (fdr_df)

```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec_fash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_fash <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_fash <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "ND")
  fdr_vec_fash[i] <- num_false_discoveries_fash / num_discoveries_fash
}

# Create a data frame for plotting
fdr_df_fash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_fash, method = "FASH")

# Calculate true FDR for MASH
fdr_vec_mash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "ND")
  fdr_vec_mash[i] <- num_false_discoveries_mash / num_discoveries_mash
}

# Create a data frame for plotting
fdr_df_mash_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_mash, method = "MASH")
fdr_df_fash_for_plotting$method <- "FASH"

# Combine data for plotting
fdr_df_combined <- rbind(fdr_df_fash_for_plotting, fdr_df_mash_for_plotting)

# Plot the nominal FDR vs true FDR for both methods
ggplot(fdr_df_combined, aes(x = threshold, y = true_fdr, color = method)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Nominal False Discovery Rate", y = "Actual False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  geom_hline(yintercept = size_each[1]/N, linetype = "dashed", color = "red") +
  ggtitle("Nominal FDR vs Actual FDR Curves for MASH and FASH")
```


Compare the number of false discoveries against the number of total discoveries for both methods:

```{r}
threshold_vec <- seq(0, 1, by = 0.0001)
num_discoveries_vec_fash <- numeric(length(threshold_vec))
num_false_discoveries_vec_fash <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_fash[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec_fash[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "ND")
}
num_discoveries_df_fash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_fash, num_false_discoveries = num_false_discoveries_vec_fash, method = "FASH")

num_discoveries_vec_mash <- numeric(length(threshold_vec))
num_false_discoveries_vec_mash <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_mash[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec_mash[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "ND")
}

num_discoveries_df_mash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_mash, num_false_discoveries = num_false_discoveries_vec_mash, method = "MASH")

num_discoveries_df_combined <- rbind(num_discoveries_df_fash, num_discoveries_df_mash)

ggplot(num_discoveries_df_combined, aes(x = num_discoveries, y = num_false_discoveries, color = method)) +
  geom_line(aes(linetype = method)) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("Number of False Discoveries vs Number of Discoveries for MASH and FASH") +
  coord_cartesian(xlim = c(0, size_each[1]), ylim = c(0, size_each[1]))
```


Compare the FDR vs Power for both methods:

```{r}
# Calculate the power for FASH
num_discoveries_df_fash$power <- (num_discoveries_df_fash$num_discoveries-num_discoveries_df_fash$num_false_discoveries)/(N - size_each[1])

# Calculate the power for MASH
num_discoveries_df_mash$power <- (num_discoveries_df_mash$num_discoveries - num_discoveries_df_mash$num_false_discoveries)/(N - size_each[1])

# Combine the data for plotting
num_discoveries_df_combined <- rbind(num_discoveries_df_fash, num_discoveries_df_mash)

# Plot the FDR vs Power for both methods
ggplot(num_discoveries_df_combined, aes(x = threshold, y = power, color = method)) +
  geom_line(aes(linetype = method)) +
  # geom_point() +
  labs(x = "FDR", y = "Power") +
  theme_minimal() +
  ggtitle("FDR vs Power for MASH and FASH") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))
```





## **Comparison with Oracle Bayes**

Finally, we also compare the performance of FASH with the oracle Bayes method that uses the true prior to produce inference.

```{r eval=FALSE}
true_prior_weight <- prior_weight[,1:2]
true_prior_weight$prior_weight <- c(0.6, 0.1, 0.1, 0.075, 0.075, 0.05, 0, 0, 0, 0, 0)
num_datasets <- length(data_sim_list)
num_weights <- sum(true_prior_weight$prior_weight != 0)
true_posterior_weights_matrix <- matrix(nrow = num_datasets, ncol = num_weights)

# Loop through each dataset and perform fitting
fitted_datasets <- list()

pb <- txtProgressBar(min = 0, max = N, style = 3)
for (i in seq_along(data_sim_list)) {
  setTxtProgressBar(pb, i)
  dataset <- data_sim_list[[i]]
  fit_result_final <- fit_ospline_with_prior2(
    num_cores = 1,
    x = dataset$x,
    y = dataset$y,
    num_knots = 50,
    prior_weight = true_prior_weight,
    betaprec = 1,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets[[i]] <- aggregate_fit_with_prior(x = dataset$x, fit_results_with_prior = fit_result_final)$summary_df
  true_posterior_weights_matrix[i, ] <- fit_result_final$posterior_weights[, "posterior_weight"]
}
colnames(true_posterior_weights_matrix) <- paste(as.character(fit_result_final$posterior_weights[, "p"]),
                                            as.character(fit_result_final$posterior_weights[, "psd_iwp"]), sep = "_")
save(true_posterior_weights_matrix, file = paste0(result_dir, "/true_posterior_weights_matrix.rda"))
```

```{r}
true_prior_weight <- prior_weight[,1:2]
true_prior_weight$prior_weight <- c(0.6, 0.1, 0.1, 0.075, 0.075, 0.05, 0, 0, 0, 0, 0)
num_datasets <- length(data_sim_list)
num_weights <- sum(true_prior_weight$prior_weight != 0)
load(paste0(result_dir, "/true_posterior_weights_matrix.rda"))
```

```{r}
# compute the posterior mean
psd_iwp_vec_effective <- true_prior_weight$psd_iwp[true_prior_weight$prior_weight != 0]
posterior_mean <- rowSums(true_posterior_weights_matrix * matrix(psd_iwp_vec_effective, nrow = num_datasets, ncol = length(psd_iwp_vec_effective), byrow = T))
PSD_data$OB <- posterior_mean
```

First, compare the estimation accuracy of the PSD $\sigma$:

```{r}
# Combine MLE and EB data into a single dataframe
PSD_data_long <- PSD_data %>%
  pivot_longer(cols = c("MLE", "EB", "OB"), names_to = "Method", values_to = "Estimate")

# Calculate the Mean Squared Error (MSE) for each method and SD
plot_combined <- PSD_data_long %>%
  group_by(truth, sd, Method) %>%
  summarize(MSE = mean((Estimate - truth)^2)) %>%
  ggplot(aes(x = truth, y = MSE, color = Method)) + 
  geom_point() + 
  geom_line() +
  facet_wrap(~ sd, scales = "free_y") +  # Facet by SD
  ggtitle("Comparison of MSE across SD values") + 
  labs(x = "Truth", y = "MSE", color = "Method") +
  ylim(0, 0.15) +
  theme_minimal()

# Print the combined plot
print(plot_combined)
```


Then, let's check the calibration of FDR and its corresponding power.

Compute the local false discovery rate (lfdr):
```{r}
set.seed(123)
lfdr_ob <- true_posterior_weights_matrix[,1]
fdr_df_ob <- data.frame(eQTL = 1:length(lfdr_ob), fdr = lfdr_ob, type = rep(c("ND","D"), times = c(size_each[1],N-size_each[1])))
fdr_df_ob <- fdr_df_ob[order(fdr_df_ob$fdr), ] # ordering it
fdr_df_ob$cumulative_fdr <- cumsum(fdr_df_ob$fdr)/seq_along(fdr_df_ob$fdr)
fdr_df_ob$rank <- 1:length(lfdr_ob)
```

Compare the number of false discoveries with the number of discoveries at different level of threshold, where number of discoveries is plotted against number of false discoveries:

```{r}
threshold_vec <- seq(0, 1, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fdr_df_ob$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fdr_df_ob$cumulative_fdr <= threshold_vec[i] & fdr_df_ob$type == "ND")
}
num_discoveries_df <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df, aes(x = (num_discoveries), y = (num_false_discoveries))) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("OB: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, size_each[1]), ylim = c(0, size_each[1]))
```


Compare FASH, OB and MASH in terms of FDR calibration:

```{r}
# Create a data frame for plotting
fdr_vec_ob <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  fdr_vec_ob[i] <- num_false_discoveries_vec[i] / num_discoveries_vec[i]
}

# Create a data frame for plotting
fdr_df_ob_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_ob, method = "OB")

# Combine data for plotting
fdr_df_combined <- rbind(fdr_df_fash_for_plotting, fdr_df_mash_for_plotting, fdr_df_ob_for_plotting)

# Plot the nominal FDR vs true FDR for both methods
ggplot(fdr_df_combined, aes(x = threshold, y = true_fdr, color = method)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Nominal False Discovery Rate", y = "Actual False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  geom_hline(yintercept = size_each[1]/N, linetype = "dashed", color = "red") +
  ggtitle("Nominal FDR vs Actual FDR Curves for All Methods")
```

Compare FASH, OB and MASH in terms of number of discoveries:

```{r}
threshold_vec <- seq(0, 1, by = 0.0001)
num_discoveries_vec_ob <- numeric(length(threshold_vec))
num_false_discoveries_vec_ob <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_ob[i] <- sum(fdr_df_ob$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec_ob[i] <- sum(fdr_df_ob$cumulative_fdr <= threshold_vec[i] & fdr_df_ob$type == "ND")
}

num_discoveries_vec_ob <- numeric(length(threshold_vec))
num_false_discoveries_vec_ob <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_ob[i] <- sum(fdr_df_ob$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec_ob[i] <- sum(fdr_df_ob$cumulative_fdr <= threshold_vec[i] & fdr_df_ob$type == "ND")
}

num_discoveries_df_ob <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_ob, num_false_discoveries = num_false_discoveries_vec_ob, method = "OB")

# Calculate the power for OB
num_discoveries_df_ob$power <- (num_discoveries_df_ob$num_discoveries-num_discoveries_df_ob$num_false_discoveries)/(N - size_each[1])

num_discoveries_df_combined <- rbind(num_discoveries_df_fash, num_discoveries_df_mash, num_discoveries_df_ob)

ggplot(num_discoveries_df_combined, aes(x = num_discoveries, y = num_false_discoveries, color = method)) +
  geom_line(aes(linetype = method)) +
  # geom_point(size = 0.01) + 
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("Number of False Discoveries vs Number of Discoveries for All Methods") +
  coord_cartesian(xlim = c(0, size_each[1]), ylim = c(0, size_each[1]))
```

Compare FASH, OB and MASH in terms of power:

```{r}
# Plot the FDR vs Power for both methods
ggplot(num_discoveries_df_combined, aes(x = threshold, y = power, color = method)) +
  geom_line(aes(linetype = method)) +
  # geom_point() +
  labs(x = "FDR", y = "Power") +
  theme_minimal() +
  ggtitle("FDR vs Power for All Methods") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1))
```

Overall, FASH seems to perform quite close to the oracle Bayes (OB) approach that utilizes the true prior.






