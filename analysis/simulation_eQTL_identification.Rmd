---
title: "Identifying dynamic eQTLs using FASH"
author: "Ziang Zhang"
date: "2024-05-19"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## **Context**

We consider the effect size estimate of $1000$ eQTLs measured in day $t = 1$ to day $t = 16$:

- there are $700$ eQTLs that are not dynamic, i.e., the effect size is constant over time.
- there are $200$ eQTLs that are linear dynamic, i.e., the effect size is changing linearly over time.
- there are $100$ eQTLs that are non-linear dynamic, i.e., the effect size is changing non-linearly over time.

For simplicity, let's assume the SE of effect estimate is constant over time: $\text{SE}(\hat{\beta}_{i}(t_j)) = \sigma_i, \forall j$ and hence: $$\hat{\beta}_i(t_j) \sim N(\beta_i(t_j),\sigma^2).$$

We consider the following prior:
$$\beta_i(t) = \sum_{j=0}^K \pi_j g_j(t),$$
where each $g_j(t)$ is a GP defined by:
$$Lg_j(t) = \sigma_j W(t),$$
with $L = \frac{d}{dt}$ and $W(t)$ denotes the Gaussian white noise.
In this case, the base model is the space of constant functions $\text{span}\{1\}$. 
The size of $\sigma_j$ controls the strength of the shrinkage towards the base model.
The first component has $\sigma_0 = 0$ to represent the base model.

## **Data**

Load the required libraries and set the seed.
```{r}
library(BayesGP)
library(TMB)
library(Matrix)
library(splines)
library(parallel)
library(ggplot2)
library(reshape2)
library(mixsqp)
library(tidyverse)
library(mashr)
cpp_dir <- paste0(getwd(), "/code/cpp")
fig_dir <- paste0(getwd(), "/output/simulation_eQTL_test")
result_dir <- paste0(getwd(), "/output/simulation_eQTL_test")
function_dir <- paste0(getwd(), "/code/function")
source(paste0(function_dir, "/functions_fitting_Gaussian_eQTL_test.R"))
source(paste0(function_dir, "/functions_simulation_eQTL_test.R"))
compile(paste0(cpp_dir, "/Gaussian_theta_known.cpp"))
compile(paste0(cpp_dir, "/Gaussian_just_fixed.cpp"))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_theta_known")))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_just_fixed")))
```

```{r}
num_cores <- 4
set.seed(123)
N <- 1000
sigma <- sample(c(0.1, 0.5, 1), size = N, replace = TRUE, prob = c(1/3,1/3,1/3))
set.seed(123)
```

First, we simulate $700$ non-dynamic eQTLs:

```{r}
sizeA <- 700
data_sim_list_A <- lapply(1:sizeA, function(i) simulate_process(sd_poly = 1, type = "nondynamic", sd = sigma[i]))
par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(data_sim_list_A[[i]]$x, data_sim_list_A[[i]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list_A[[i]]$truef, data_sim_list_A[[i]]$y),
       ylab = "Effect size", main = paste0("eQTL ", i))
  points(data_sim_list_A[[i]]$x, data_sim_list_A[[i]]$y, col = "blue")
}
par(mfrow = c(1, 1))
```

Then, we simulate $200$ dynamic eQTLs with linear dynamics:

```{r}
sizeB <- 200
data_sim_list_B <- lapply(1:sizeB, function(i) simulate_process(sd_poly = 0.1, type = "linear", sd = sigma[i + sizeA]))
par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(data_sim_list_B[[i]]$x, data_sim_list_B[[i]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list_B[[i]]$truef, data_sim_list_B[[i]]$y),
       ylab = "Effect size", main = paste0("eQTL ", i))
  points(data_sim_list_B[[i]]$x, data_sim_list_B[[i]]$y, col = "blue")
}
par(mfrow = c(1, 1))
```

Finally, simulate $100$ non-linear dynamic eQTLs.

```{r}
sizeC <- 100
data_sim_list_C <- lapply(1:sizeC, function(i) simulate_process(sd_poly = 0.1, type = "nonlinear", sd = sigma[i + sizeA + sizeB], sd_fun = 1))
par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(data_sim_list_C[[i]]$x, data_sim_list_C[[i]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list_C[[i]]$truef, data_sim_list_C[[i]]$y),
       ylab = "Effect size", main = paste0("eQTL ", i))
  points(data_sim_list_C[[i]]$x, data_sim_list_C[[i]]$y, col = "blue")
}
par(mfrow = c(1, 1))

datasets <- c(data_sim_list_A, data_sim_list_B, data_sim_list_C)
sigma <- unlist(lapply(datasets, function(x) unique(x$sd)))
```

## **Hypothesis Testing**

### **Which eQTLs are non-dynamic?**

Let's consider the question of identifying dynamic eQTLs. 
Based on the prior we used, an eQTL should be considered non-dynamic if the posterior weight $\pi_0$ of the first component is close to $1$.

First, we compute the L matrix:

```{r, eval=FALSE}
set.seed(123)
p_vec <- 1
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
L_vecs <- list()
# create a progress bar
pb <- txtProgressBar(min = 0, max = length(datasets), style = 3)
for (i in 1:length(datasets)) {
  setTxtProgressBar(pb, i)
  L_vecs[[i]] <- compute_log_likelihood_ospline_seq2(
    x = datasets[[i]]$x,
    y = datasets[[i]]$y,
    p = p_vec,
    num_knots = 16,
    psd_iwp_vector = psd_iwp_vec,
    pred_step = 1,
    betaprec = 0.001,
    sd_gaussian = sigma[i]
  )
}
L_matrix <- do.call(rbind, L_vecs)
save(L_matrix, file = paste0(result_dir, "/L_matrix.rda"))
```

```{r, echo=FALSE}
set.seed(123)
p_vec <- 1
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
load(paste0(result_dir, "/L_matrix.rda"))
```

Based on the L-matrix, we optimize the prior weights through EB:

```{r}
fit.sqp <- mixsqp(L = L_matrix, log = TRUE)
numiter <- nrow(fit.sqp$progress)
plot(1:numiter,fit.sqp$progress$objective,type = "b",
     pch = 20,lwd = 2,xlab = "SQP iteration",
     ylab = "objective",xaxp = c(1,numiter,numiter - 1))
prior_weight <- data.frame(p = rep(p_vec, each = length(psd_iwp_vec)), psd_iwp = psd_iwp_vec, prior_weight = fit.sqp$x)
```

```{r}
head(prior_weight)
```

The estimated overall proportion of non-dynamic eQTLs is:

```{r}
prior_weight %>%
  filter(psd_iwp == 0) %>%
  pull(prior_weight)
```

With the estimated prior, we can now perform the posterior inference for each dataset:

```{r, eval=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
posterior_weights_matrix <- matrix(nrow = num_datasets, ncol = num_weights)

# Loop through each dataset and perform fitting
fitted_datasets <- list()
# start a progress bar
pb <- txtProgressBar(min = 0, max = num_datasets, style = 3)
for (i in seq_along(datasets)) {
  setTxtProgressBar(pb, i)
  dataset <- datasets[[i]]
  fit_result_final <- fit_ospline_with_prior2(
    num_cores = 1,
    x = dataset$x,
    y = dataset$y,
    num_knots = 16,
    prior_weight = prior_weight,
    betaprec = 0.001,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets[[i]] <- aggregate_fit_with_prior(x = dataset$x, fit_results_with_prior = fit_result_final)$summary_df
  posterior_weights_matrix[i, ] <- fit_result_final$posterior_weights[, "posterior_weight"]
}
colnames(posterior_weights_matrix) <- paste(as.character(fit_result_final$posterior_weights[, "p"]),
                                            as.character(fit_result_final$posterior_weights[, "psd_iwp"]), sep = "_")
save(posterior_weights_matrix, file = paste0(result_dir, "/posterior_weights_matrix.rda"))
save(fitted_datasets, file = paste0(result_dir, "/fitted_datasets.rda"))
```

```{r, echo=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
load(paste0(result_dir, "/posterior_weights_matrix.rda"))
load(paste0(result_dir, "/fitted_datasets.rda"))
```

We can visualize the posterior weights for each dataset:

```{r}
posterior_weights_df <- as.data.frame(posterior_weights_matrix)
posterior_weights_df$id <- c(1:length(sigma))
melted_data <- melt(posterior_weights_df, id.vars = "id")
melted_data$variable2 <- sub("_.*", "", melted_data$variable)
melted_data$variable3 <- (round(as.numeric(sub("*._", "", melted_data$variable)), 3))

ggplot(melted_data, aes(x = id, y = value, fill = variable3)) +
  geom_bar(stat = "identity") +
  labs(x = "Observation ID", y = "Weight", fill = "PSD") +
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "blue") +
  ggtitle("FASH: Structure Plot of Posterior Weights") +
  coord_flip() 
```

#### *Using FASH*

Compute the local false discovery rate (lfda):
```{r}
set.seed(123)
lfda <- posterior_weights_matrix[,1]
fda_df <- data.frame(eQTL = 1:length(lfda), fda = lfda, type = rep(c("A", "B", "C"), times = c(sizeA, sizeB, sizeC)))
fda_df <- fda_df[order(fda_df$fda), ] # ordering it
fda_df$cumulative_fda <- cumsum(fda_df$fda)/seq_along(fda_df$fda)
fda_df$rank <- 1:length(lfda)
```

```{r}
ggplot(fda_df, aes(x = 1:length(lfda), y = cumulative_fda, col = type)) +
  geom_line() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDA", col = "Type") +
  theme_minimal() +
  ggtitle("Cumulative FDA Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```

How many false discoveries are there?
```{r}
alpha <- 0.05
num_discoveries <- sum(fda_df$cumulative_fda <= alpha)
num_false_discoveries <- sum(fda_df$cumulative_fda <= alpha & fda_df$type == "A")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
```

Plot the curve of nominal false discovery rate (threshold) against the actual true false discovery rate:
```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0.01, 0.99, by = 0.01)
fdr_vec <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries <- sum(fda_df$cumulative_fda <= threshold_vec[i])
  num_false_discoveries <- sum(fda_df$cumulative_fda <= threshold_vec[i] & fda_df$type == "A")
  fdr_vec[i] <- num_false_discoveries / num_discoveries
}

# Create a data frame for plotting
fdr_df_fash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec)

# Plot the nominal FDR vs true FDR for FASH
ggplot(fdr_df_fash, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Nominal False Discovery Rate", y = "True False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs True FDR Curve for FASH")
```

Also compare the number of false discoveries with the number of discoveries at different level of threshold, where number of discoveries is plotted against number of false discoveries:

```{r}
threshold_vec <- seq(0.01, 0.99, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fda_df$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fda_df$cumulative_fda <= threshold_vec[i] & fda_df$type == "A")
}
num_discoveries_df <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df, aes(x = (num_discoveries), y = num_false_discoveries)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("FASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 1000))

```

#### *Using MASH*

Now, let's do the comparison with the default implementation of MASH:

```{r}
fitted_datasets_mash <- list()

# Produce a huge data-matrix, the i-th row being dataset[[i]]$y
all_data_matrix <- do.call(rbind, lapply(datasets, function(x) x$y))
SE_matrix <- matrix(nrow = nrow(all_data_matrix), ncol = ncol(all_data_matrix), sigma)

# now use mashr:
mash_data <-  mashr::mash_set_data(all_data_matrix, SE_matrix)
m.1by1 = mashr::mash_1by1(mash_data)
strong = mashr::get_significant_results(m.1by1, 0.05)
# keep the top 10%
strong <- strong[1:round(0.1*length(strong))]
U.pca = mashr::cov_pca(mash_data, 5, subset = strong)
U.ed = cov_ed(mash_data, U.pca, subset=strong)
U.c = cov_canonical(mash_data)  
m   = mash(mash_data, c(U.c,U.ed))
```

```{r}
mash_post <- m$posterior_weights
## extract the colnames start with "equal_effects"
lfda_mash <- mash_post[, c(1, grep("equal_effects", colnames(mash_post)))]
# sum each row
lfda_mash <- rowSums(lfda_mash)

fda_df_mash <- data.frame(eQTL = 1:length(lfda), fda = lfda_mash, type = rep(c("A", "B", "C"), times = c(sizeA, sizeB, sizeC)))

fda_df_mash <- fda_df_mash[order(fda_df_mash$fda), ] # ordering it
fda_df_mash$cumulative_fda <- cumsum(fda_df_mash$fda)/seq_along(fda_df_mash$fda)
fda_df_mash$rank <- 1:length(lfda_mash)
```

How many false discoveries are there?

```{r}
alpha <- 0.05
num_discoveries <- sum(fda_df_mash$cumulative_fda <= alpha)
num_false_discoveries <- sum(fda_df_mash$cumulative_fda <= alpha & fda_df_mash$type == "A")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
```

```{r}
ggplot(fda_df_mash, aes(x = rank, y = cumulative_fda, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDA", col = "Type") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 1)) +
  ggtitle("Cumulative FDA Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```

```{r}
# Calculate true FDR for MASH
threshold_vec <- seq(0.01, 0.99, by = 0.01)
fdr_vec_mash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_mash <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_mash <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i] & fda_df_mash$type == "A")
  fdr_vec_mash[i] <- num_false_discoveries_mash / num_discoveries_mash
}

# Create a data frame for plotting
fdr_df_mash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_mash)

# Plot the nominal FDR vs true FDR for MASH
ggplot(fdr_df_mash, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Nominal False Discovery Rate", y = "True False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs True FDR Curve for MASH")
```

Plot the number of false discoveries against the number of total discoveries for MASH:

```{r}
threshold_vec <- seq(0.01, 0.99, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i] & fda_df_mash$type == "A")
}
num_discoveries_df_mash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df_mash, aes(x = num_discoveries, y = num_false_discoveries)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("MASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 1000))
```


Compare MASH (fda_df_mash) and FASH (fda_df)

```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0.01, 0.99, by = 0.01)
fdr_vec_fash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_fash <- sum(fda_df$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_fash <- sum(fda_df$cumulative_fda <= threshold_vec[i] & fda_df$type == "A")
  fdr_vec_fash[i] <- num_false_discoveries_fash / num_discoveries_fash
}

# Create a data frame for plotting
fdr_df_fash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_fash, method = "FASH")

# Calculate true FDR for MASH
fdr_vec_mash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_mash <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_mash <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i] & fda_df_mash$type == "A")
  fdr_vec_mash[i] <- num_false_discoveries_mash / num_discoveries_mash
}

# Create a data frame for plotting
fdr_df_mash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_mash, method = "MASH")

# Combine data for plotting
fdr_df_combined <- rbind(fdr_df_fash, fdr_df_mash)

# Plot the nominal FDR vs true FDR for both methods
ggplot(fdr_df_combined, aes(x = threshold, y = true_fdr, color = method)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Nominal False Discovery Rate", y = "True False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs True FDR Curves for MASH and FASH")
```

Compare the number of false discoveries against the number of total discoveries for both methods:

```{r}
threshold_vec <- c(0.001,seq(0.01, 0.99, by = 0.01), 0.999)
num_discoveries_vec_fash <- numeric(length(threshold_vec))
num_false_discoveries_vec_fash <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_fash[i] <- sum(fda_df$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_vec_fash[i] <- sum(fda_df$cumulative_fda <= threshold_vec[i] & fda_df$type == "A")
}
num_discoveries_df_fash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_fash, num_false_discoveries = num_false_discoveries_vec_fash, method = "FASH")

num_discoveries_vec_mash <- numeric(length(threshold_vec))
num_false_discoveries_vec_mash <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_mash[i] <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i])
  num_false_discoveries_vec_mash[i] <- sum(fda_df_mash$cumulative_fda <= threshold_vec[i] & fda_df_mash$type == "A")
}

num_discoveries_df_mash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_mash, num_false_discoveries = num_false_discoveries_vec_mash, method = "MASH")

num_discoveries_df_combined <- rbind(num_discoveries_df_fash, num_discoveries_df_mash)

ggplot(num_discoveries_df_combined, aes(x = num_discoveries, y = num_false_discoveries, color = method)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("Number of False Discoveries vs Number of Discoveries for MASH and FASH") +
  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 1000))

```

