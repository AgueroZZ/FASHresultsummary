---
title: "Identifying dynamic eQTLs using FASH"
author: "Ziang Zhang"
date: "2024-05-19"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

## **Setup**

We consider the effect size estimate of $10,000$ eQTLs measured in day $t = 1$ to day $t = 16$:

- there are $7000$ eQTLs that are not dynamic, i.e., the effect size is constant over time.
- there are $2000$ eQTLs that are linear dynamic, i.e., the effect size is changing linearly over time.
- there are $1000$ eQTLs that are non-linear dynamic, i.e., the effect size is changing non-linearly over time.

For simplicity, let's assume the SE of effect estimate is constant over time: $\text{SE}(\hat{\beta}_{i}(t_j)) = \sigma_i, \forall j$ and hence: $$\hat{\beta}_i(t_j) \sim N(\beta_i(t_j),\sigma^2).$$


## **Data**

Load the required libraries and set the seed.
```{r echo=FALSE}
library(BayesGP)
library(TMB)
library(Matrix)
library(splines)
library(parallel)
library(ggplot2)
library(reshape2)
library(mixsqp)
library(tidyverse)
library(mashr)
cpp_dir <- paste0(getwd(), "/code/cpp")
fig_dir <- paste0(getwd(), "/output/simulation_eQTL_test")
result_dir <- paste0(getwd(), "/output/simulation_eQTL_test")
function_dir <- paste0(getwd(), "/code/function")
source(paste0(function_dir, "/functions_fitting_Gaussian_eQTL_test.R"))
source(paste0(function_dir, "/functions_simulation_eQTL_test.R"))
compile(paste0(cpp_dir, "/Gaussian_theta_known.cpp"))
compile(paste0(cpp_dir, "/Gaussian_just_fixed.cpp"))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_theta_known")))
dyn.load(TMB::dynlib(paste0(cpp_dir, "/Gaussian_just_fixed")))
```

```{r}
num_cores <- 4
set.seed(123)
N <- 10000
sigma <- sample(c(0.1, 0.5, 1), size = N, replace = TRUE, prob = c(1/3,1/3,1/3))
set.seed(123)
```

First, we simulate $7000$ non-dynamic eQTLs:

```{r}
sizeA <- 7000
data_sim_list_A <- lapply(1:sizeA, function(i) simulate_process(sd_poly = 1, type = "nondynamic", sd = sigma[i]))
par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(data_sim_list_A[[i]]$x, data_sim_list_A[[i]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list_A[[i]]$truef, data_sim_list_A[[i]]$y),
       ylab = "Effect size", main = paste0("eQTL ", i))
  points(data_sim_list_A[[i]]$x, data_sim_list_A[[i]]$y, col = "blue")
}
par(mfrow = c(1, 1))
```

Then, we simulate $2000$ dynamic eQTLs with linear dynamics:

```{r}
sizeB <- 2000
data_sim_list_B <- lapply(1:sizeB, function(i) simulate_process(sd_poly = 0.1, type = "linear", sd = sigma[i + sizeA]))
par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(data_sim_list_B[[i]]$x, data_sim_list_B[[i]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list_B[[i]]$truef, data_sim_list_B[[i]]$y),
       ylab = "Effect size", main = paste0("eQTL ", i))
  points(data_sim_list_B[[i]]$x, data_sim_list_B[[i]]$y, col = "blue")
}
par(mfrow = c(1, 1))
```

Finally, simulate $1000$ non-linear dynamic eQTLs.

```{r}
sizeC <- 1000
data_sim_list_C <- lapply(1:sizeC, function(i) simulate_process(sd_poly = 0.1, type = "nonlinear", sd = sigma[i + sizeA + sizeB], sd_fun = 1))
par(mfrow = c(2, 2))
for (i in 1:4) {
  plot(data_sim_list_C[[i]]$x, data_sim_list_C[[i]]$truef, 
       type = "l", col = "red", xlab = "Time",
       ylim = range(data_sim_list_C[[i]]$truef, data_sim_list_C[[i]]$y),
       ylab = "Effect size", main = paste0("eQTL ", i))
  points(data_sim_list_C[[i]]$x, data_sim_list_C[[i]]$y, col = "blue")
}
par(mfrow = c(1, 1))

datasets <- c(data_sim_list_A, data_sim_list_B, data_sim_list_C)
sigma <- unlist(lapply(datasets, function(x) unique(x$sd)))
```

## **Hypothesis Testing**

We consider the following prior:
$$\beta_i(t) = \sum_{j=0}^K \pi_j g_j(t),$$
where each $g_j(t)$ is a GP defined by:
$$Lg_j(t) = \sigma_j W(t).$$

The choice of $L$ specifies the base model (i.e. $\text{Null}\{L\}$) for which the effect function $\beta_i(t)$ is shrunk towards, and the size of $\sigma_j$ (inversely) controls the strength of the shrinkage.
The first mixture component has $\sigma_0 = 0$ to represent the exact base model.

### **Which eQTLs are non-dynamic?**

Let's consider the question of identifying dynamic eQTLs. 
The $i$th eQTL is considered non-dynamic if its effect $\beta_i(t)$ is constant over time.
To use the FASH method to identify non-dynamic eQTLs, we consider $L = \frac{d}{dt}$, which corresponds to a constant base model $\text{Null}\{L\} = \text{span}\{1\}$.

Based on the prior we used, the $i$th eQTL should be considered non-dynamic if the posterior weight $\pi_0$ of the first mixture component is close to $1$, which implies $\beta_i(t)$ has high posterior probability of being in $\text{Null}\{L\}$.

#### *Implementing FASH *

First, we compute the L matrix:

```{r, eval=FALSE}
set.seed(123)
p_vec <- 1
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
L_vecs <- list()
# create a progress bar
pb <- txtProgressBar(min = 0, max = length(datasets), style = 3)
for (i in 1:length(datasets)) {
  setTxtProgressBar(pb, i)
  L_vecs[[i]] <- compute_log_likelihood_ospline_seq2(
    x = datasets[[i]]$x,
    y = datasets[[i]]$y,
    p = p_vec,
    num_knots = 16,
    psd_iwp_vector = psd_iwp_vec,
    pred_step = 1,
    betaprec = 0.001,
    sd_gaussian = sigma[i]
  )
}
L_matrix <- do.call(rbind, L_vecs)
save(L_matrix, file = paste0(result_dir, "/L_matrix.rda"))
```

```{r, echo=FALSE}
set.seed(123)
p_vec <- 1
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
load(paste0(result_dir, "/L_matrix.rda"))
```

Based on the L-matrix, we optimize the prior weights through EB:

```{r}
fit.sqp <- mixsqp(L = L_matrix, log = TRUE)
numiter <- nrow(fit.sqp$progress)
plot(1:numiter,fit.sqp$progress$objective,type = "b",
     pch = 20,lwd = 2,xlab = "SQP iteration",
     ylab = "objective",xaxp = c(1,numiter,numiter - 1))
prior_weight <- data.frame(p = rep(p_vec, each = length(psd_iwp_vec)), psd_iwp = psd_iwp_vec, prior_weight = fit.sqp$x)
```

```{r}
head(prior_weight)
```

The estimated overall proportion of non-dynamic eQTLs is:

```{r}
prior_weight %>%
  filter(psd_iwp == 0) %>%
  pull(prior_weight)
```

With the estimated prior, we can now perform the posterior inference for each dataset:

```{r, eval=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
posterior_weights_matrix <- matrix(nrow = num_datasets, ncol = num_weights)

# Loop through each dataset and perform fitting
fitted_datasets <- list()
# start a progress bar
pb <- txtProgressBar(min = 0, max = num_datasets, style = 3)
for (i in seq_along(datasets)) {
  setTxtProgressBar(pb, i)
  dataset <- datasets[[i]]
  fit_result_final <- fit_ospline_with_prior2(
    num_cores = 1,
    x = dataset$x,
    y = dataset$y,
    num_knots = 16,
    prior_weight = prior_weight,
    betaprec = 0.001,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets[[i]] <- aggregate_fit_with_prior(x = dataset$x, fit_results_with_prior = fit_result_final)$summary_df
  posterior_weights_matrix[i, ] <- fit_result_final$posterior_weights[, "posterior_weight"]
}
colnames(posterior_weights_matrix) <- paste(as.character(fit_result_final$posterior_weights[, "p"]),
                                            as.character(fit_result_final$posterior_weights[, "psd_iwp"]), sep = "_")
save(posterior_weights_matrix, file = paste0(result_dir, "/posterior_weights_matrix.rda"))
save(fitted_datasets, file = paste0(result_dir, "/fitted_datasets.rda"))
```

```{r, echo=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
load(paste0(result_dir, "/posterior_weights_matrix.rda"))
load(paste0(result_dir, "/fitted_datasets.rda"))
```

We can visualize the posterior weights for each dataset:

```{r}
posterior_weights_df <- as.data.frame(posterior_weights_matrix)
posterior_weights_df$id <- c(1:length(sigma))
melted_data <- melt(posterior_weights_df, id.vars = "id")
melted_data$variable2 <- sub("_.*", "", melted_data$variable)
melted_data$variable3 <- (round(as.numeric(sub("*._", "", melted_data$variable)), 3))

ggplot(melted_data, aes(x = id, y = value, fill = variable3)) +
  geom_bar(stat = "identity") +
  labs(x = "Observation ID", y = "Weight", fill = "PSD") +
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "blue") +
  ggtitle("FASH: Structure Plot of Posterior Weights") +
  coord_flip() 
```

#### *FDR Computed Using FASH*

Compute the local false discovery rate (lfdr):
```{r}
set.seed(123)
lfdr <- posterior_weights_matrix[,1]
fdr_df <- data.frame(eQTL = 1:length(lfdr), fdr = lfdr, type = rep(c("A", "B", "C"), times = c(sizeA, sizeB, sizeC)))
fdr_df <- fdr_df[order(fdr_df$fdr), ] # ordering it
fdr_df$cumulative_fdr <- cumsum(fdr_df$fdr)/seq_along(fdr_df$fdr)
fdr_df$rank <- 1:length(lfdr)
```

```{r}
ggplot(fdr_df, aes(x = 1:length(lfdr), y = cumulative_fdr, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDR", col = "Type") +
  theme_minimal() +
  ggtitle("FASH:Cumulative FDR Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```

How many false discoveries are there?
```{r}
alpha <- 0.05
num_discoveries <- sum(fdr_df$cumulative_fdr <= alpha)
num_false_discoveries <- sum(fdr_df$cumulative_fdr <= alpha & fdr_df$type == "A")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
```

Plot the curve of nominal false discovery rate (threshold) against the actual false discovery rate:
```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "A")
  fdr_vec[i] <- num_false_discoveries / num_discoveries
}

# Create a data frame for plotting
fdr_df_fash_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec)

# Plot the nominal FDR vs true FDR for FASH
ggplot(fdr_df_fash_for_plotting, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "(Nominal) False Discovery Rate", y = "(Actual) False Discovery Rate") +
  theme_minimal() +
  geom_hline(yintercept = sizeA/N, linetype = "dashed", color = "red") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs Actual FDR Curve for FASH")
```

Also compare the number of false discoveries with the number of discoveries at different level of threshold, where number of discoveries is plotted against number of false discoveries:

```{r}
threshold_vec <- seq(0, 1, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "A")
}
num_discoveries_df <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df, aes(x = (num_discoveries), y = (num_false_discoveries))) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("FASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, 3000), ylim = c(0, 3000))

```

#### *FDR Computed Using MASH*

Now, let's do the comparison with the default implementation of MASH:

```{r, eval=FALSE}
fitted_datasets_mash <- list()

# Produce a huge data-matrix, the i-th row being dataset[[i]]$y
all_data_matrix <- do.call(rbind, lapply(datasets, function(x) x$y))
SE_matrix <- matrix(nrow = nrow(all_data_matrix), ncol = ncol(all_data_matrix), sigma)

# now use mashr:
mash_data <-  mashr::mash_set_data(all_data_matrix, SE_matrix)
m.1by1 = mashr::mash_1by1(mash_data)
strong = mashr::get_significant_results(m.1by1, 0.05)
# keep the top 10%
strong <- strong[1:round(0.1*length(strong))]
U.pca = mashr::cov_pca(mash_data, 5, subset = strong)
U.ed = cov_ed(mash_data, U.pca, subset=strong)
U.c = cov_canonical(mash_data)  
m   = mash(mash_data, c(U.c,U.ed))
save(m, file = paste0(result_dir, "/mash_result.rda"))
```

```{r}
mash_post <- m$posterior_weights
## extract the colnames start with "equal_effects"
lfdr_mash <- mash_post[, c(1, grep("equal_effects", colnames(mash_post)))]
# sum each row
lfdr_mash <- rowSums(lfdr_mash)

fdr_df_mash <- data.frame(eQTL = 1:length(lfdr), fdr = lfdr_mash, type = rep(c("A", "B", "C"), times = c(sizeA, sizeB, sizeC)))

fdr_df_mash <- fdr_df_mash[order(fdr_df_mash$fdr), ] # ordering it
fdr_df_mash$cumulative_fdr <- cumsum(fdr_df_mash$fdr)/seq_along(fdr_df_mash$fdr)
fdr_df_mash$rank <- 1:length(lfdr_mash)
```

How many false discoveries are there?

```{r}
alpha <- 0.05
num_discoveries <- sum(fdr_df_mash$cumulative_fdr <= alpha)
num_false_discoveries <- sum(fdr_df_mash$cumulative_fdr <= alpha & fdr_df_mash$type == "A")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
```

```{r}
ggplot(fdr_df_mash, aes(x = rank, y = cumulative_fdr, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDR", col = "Type") +
  theme_minimal() +
  coord_cartesian(ylim = c(0, 1)) +
  ggtitle("Cumulative FDR Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```

```{r}
# Calculate true FDR for MASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec_mash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "A")
  fdr_vec_mash[i] <- num_false_discoveries_mash / num_discoveries_mash
}

# Create a data frame for plotting
fdr_df_mash_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_mash)

# Plot the nominal FDR vs true FDR for MASH
ggplot(fdr_df_mash_for_plotting, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "(Nominal) False Discovery Rate", y = "(Actual) False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  ggtitle("Nominal FDR vs Actual FDR Curve for MASH")
```

Plot the number of false discoveries against the number of total discoveries for MASH:

```{r}
threshold_vec <- seq(0, 1, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "A")
}
num_discoveries_df_mash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)
ggplot(num_discoveries_df_mash, aes(x = num_discoveries, y = (num_false_discoveries))) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("MASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, 3000), ylim = c(0, 3000))
```


Compare MASH (fdr_df_mash) and FASH (fdr_df)

```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec_fash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_fash <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_fash <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "A")
  fdr_vec_fash[i] <- num_false_discoveries_fash / num_discoveries_fash
}

# Create a data frame for plotting
fdr_df_fash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_fash, method = "FASH")

# Calculate true FDR for MASH
fdr_vec_mash <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_mash <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "A")
  fdr_vec_mash[i] <- num_false_discoveries_mash / num_discoveries_mash
}

# Create a data frame for plotting
fdr_df_mash_for_plotting <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec_mash, method = "MASH")
fdr_df_fash_for_plotting$method <- "FASH"

# Combine data for plotting
fdr_df_combined <- rbind(fdr_df_fash_for_plotting, fdr_df_mash_for_plotting)

# Plot the nominal FDR vs true FDR for both methods
ggplot(fdr_df_combined, aes(x = threshold, y = true_fdr, color = method)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Nominal False Discovery Rate", y = "Actual False Discovery Rate") +
  theme_minimal() +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  geom_hline(yintercept = sizeA/N, linetype = "dashed", color = "red") +
  ggtitle("Nominal FDR vs Actual FDR Curves for MASH and FASH")
```


Compare the number of false discoveries against the number of total discoveries for both methods:

```{r}
threshold_vec <- seq(0, 1, by = 0.001)
num_discoveries_vec_fash <- numeric(length(threshold_vec))
num_false_discoveries_vec_fash <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_fash[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec_fash[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type == "A")
}
num_discoveries_df_fash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_fash, num_false_discoveries = num_false_discoveries_vec_fash, method = "FASH")

num_discoveries_vec_mash <- numeric(length(threshold_vec))
num_false_discoveries_vec_mash <- numeric(length(threshold_vec))
for (i in 1:length(threshold_vec)) {
  num_discoveries_vec_mash[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec_mash[i] <- sum(fdr_df_mash$cumulative_fdr <= threshold_vec[i] & fdr_df_mash$type == "A")
}

num_discoveries_df_mash <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec_mash, num_false_discoveries = num_false_discoveries_vec_mash, method = "MASH")

num_discoveries_df_combined <- rbind(num_discoveries_df_fash, num_discoveries_df_mash)

ggplot(num_discoveries_df_combined, aes(x = num_discoveries, y = num_false_discoveries, color = method)) +
  geom_line() +
  # geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  ggtitle("Number of False Discoveries vs Number of Discoveries for MASH and FASH") +
  coord_cartesian(xlim = c(0, 10000), ylim = c(0, 7000))
```


To compare their rankings of eQTLs based on the cumulative FDR, we can compare the rank distribution of each type of eQTLs (A,B,C) based on the two methods:

```{r}
fdr_df_merge <- rbind(fdr_df %>% mutate(method = "FASH"), fdr_df_mash %>% mutate(method = "MASH"))

ggplot(fdr_df_merge, aes(x = type, y = rank, color = type)) +
  geom_boxplot() +
  labs(x = "Type", y = "Rank") +
  theme_minimal() +
  ggtitle("Rank Distribution: FASH vs MASH") +
  facet_wrap(~method)
```

**Summary:** Overall, FASH tends to make more conservative decision than MASH in terms of the discovery of non-dynamic eQTLs. 
The actual false discovery rate of FASH tends to be lower than the nominal false discovery rate, while MASH tends to have a higher actual false discovery rate than the nominal false discovery rate.
This kind of behavior is expected, as each mixture component of FASH is a L-GP prior that shrinks toward the constant base model. In contrast, the mixture components in MASH have different type of shrinkage tendencies. which gains more modeling flexibility at the cost of less informative shrinkage.
Overall, MASH does better job at controlling an actual FDR closer to the nominal level, but FASH does better job at keeping the actual FDR lower than the nominal level.
Given a threshold of FDR, MASH is able to find more discoveries than FASH, but the proportion of false discoveries is higher in MASH than in FASH.


### **Which eQTLs have non-linear dynamic?**

Now, let's consider the question of identifying non-linear dynamic eQTLs.
The $i$th eQTL is considered non-linear dynamic if its effect $\beta_i(t)$ is changing non-linearly over time.

To use the FASH method to identify non-linear dynamic eQTLs, we consider $L = \frac{d^2}{dt^2}$, which corresponds to a base model $\text{Null}\{L\} = \text{span}\{1, t\}$.

Similarly, the $i$th eQTL should be considered non-linear dynamic if the posterior weight $\pi_0$ of the first mixture component is close to $1$.

#### *Implementing FASH *

First, we compute the L matrix:

```{r, eval=FALSE}
set.seed(123)
p_vec <- 2
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
L_vecs <- list()
# create a progress bar
pb <- txtProgressBar(min = 0, max = length(datasets), style = 3)
for (i in 1:length(datasets)) {
  setTxtProgressBar(pb, i)
  L_vecs[[i]] <- compute_log_likelihood_ospline_seq2(
    x = datasets[[i]]$x,
    y = datasets[[i]]$y,
    p = p_vec,
    num_knots = 16,
    psd_iwp_vector = psd_iwp_vec,
    pred_step = 1,
    betaprec = 0.001,
    sd_gaussian = sigma[i]
  )
}
L_matrix <- do.call(rbind, L_vecs)
save(L_matrix, file = paste0(result_dir, "/L_matrix_nonlinear.rda"))
```

```{r, echo=FALSE}
set.seed(123)
p_vec <- 2
psd_iwp_vec <- sort(unique(c(0,seq(0,1, by = 0.05))))
load(paste0(result_dir, "/L_matrix_nonlinear.rda"))
```
                    
Based on the L-matrix, we optimize the prior weights through EB:

```{r}
fit.sqp <- mixsqp(L = L_matrix, log = TRUE)
numiter <- nrow(fit.sqp$progress)
plot(1:numiter,fit.sqp$progress$objective,type = "b",
     pch = 20,lwd = 2,xlab = "SQP iteration",
     ylab = "objective",xaxp = c(1,numiter,numiter - 1))
prior_weight <- data.frame(p = rep(p_vec, each = length(psd_iwp_vec)), psd_iwp = psd_iwp_vec, prior_weight = fit.sqp$x)
```

```{r}
head(prior_weight)
```

The estimated overall proportion of linear eQTLs (including non dynamic eQTLs) is:

```{r}
prior_weight %>%
  filter(psd_iwp == 0) %>%
  pull(prior_weight)
```

With the estimated prior, we can now perform the posterior inference for each dataset:

```{r, eval=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
posterior_weights_matrix <- matrix(nrow = num_datasets, ncol = num_weights)

# Loop through each dataset and perform fitting
fitted_datasets <- list()
# start a progress bar
pb <- txtProgressBar(min = 0, max = num_datasets, style = 3)
for (i in seq_along(datasets)) {
  setTxtProgressBar(pb, i)
  dataset <- datasets[[i]]
  fit_result_final <- fit_ospline_with_prior2(
    num_cores = 1,
    x = dataset$x,
    y = dataset$y,
    num_knots = 16,
    prior_weight = prior_weight,
    betaprec = 0.001,
    sd_gaussian = sigma[i],
    pred_step = 1
  )
  fitted_datasets[[i]] <- aggregate_fit_with_prior(x = dataset$x, fit_results_with_prior = fit_result_final)$summary_df
  posterior_weights_matrix[i, ] <- fit_result_final$posterior_weights[, "posterior_weight"]
}
colnames(posterior_weights_matrix) <- paste(as.character(fit_result_final$posterior_weights[, "p"]),
                                            as.character(fit_result_final$posterior_weights[, "psd_iwp"]), sep = "_")
save(posterior_weights_matrix, file = paste0(result_dir, "/posterior_weights_matrix_nonlinear.rda"))
save(fitted_datasets, file = paste0(result_dir, "/fitted_datasets_nonlinear.rda"))
```

```{r, echo=FALSE}
num_datasets <- length(datasets)
num_weights <- sum(prior_weight$prior_weight != 0)
load(paste0(result_dir, "/posterior_weights_matrix_nonlinear.rda"))
load(paste0(result_dir, "/fitted_datasets_nonlinear.rda"))
```

We can visualize the posterior weights for each dataset:

```{r}
posterior_weights_df <- as.data.frame(posterior_weights_matrix)
posterior_weights_df$id <- (c(1:length(sigma)))
melted_data <- melt(posterior_weights_df, id.vars = "id")
melted_data$variable2 <- sub("_.*", "", melted_data$variable)
melted_data$variable3 <- (round(as.numeric(sub("*._", "", melted_data$variable)), 3))

ggplot(melted_data, aes(x = id, y = value, fill = variable3)) +
  geom_bar(stat = "identity") +
  labs(x = "Observation ID", y = "Weight", fill = "PSD") +
  theme_minimal() +
  scale_fill_gradient(low = "white", high = "blue") +
  ggtitle("FASH: Structure Plot of Posterior Weights") +
  coord_flip() 
```


#### *FDR Computed Using FASH*

Compute the local false discovery rate (lfdr):
```{r}
set.seed(123)
lfdr <- posterior_weights_matrix[,1]
fdr_df <- data.frame(eQTL = 1:length(lfdr), fdr = lfdr, type = rep(c("A", "B", "C"), times = c(sizeA, sizeB, sizeC)))
fdr_df <- fdr_df[order(fdr_df$fdr), ] # ordering it
fdr_df$cumulative_fdr <- cumsum(fdr_df$fdr)/seq_along(fdr_df$fdr)
fdr_df$rank <- 1:length(lfdr)
```

```{r}
ggplot(fdr_df, aes(x = 1:length(lfdr), y = cumulative_fdr, col = type)) +
  geom_point() +
  geom_hline(yintercept = 0.05, linetype = "dashed", color = "purple") +
  labs(x = "Ordered eQTLs", y = "Cumulative FDR", col = "Type") +
  theme_minimal() +
  ggtitle("Cumulative FDR Plot") +
  scale_color_manual(values = c("red", "blue", "green"))
```

How many false discoveries are there?
```{r}
alpha <- 0.05
num_discoveries <- sum(fdr_df$cumulative_fdr <= alpha)
num_false_discoveries <- sum(fdr_df$cumulative_fdr <= alpha & fdr_df$type != "C")
true_false_discovery_rate <- num_false_discoveries/num_discoveries
true_false_discovery_rate
```

Plot the curve of nominal false discovery rate (threshold) against the actual false discovery rate:
```{r}
# Calculate true FDR for FASH
threshold_vec <- seq(0, 1, by = 0.01)
fdr_vec <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type != "C")
  fdr_vec[i] <- num_false_discoveries / num_discoveries
}

# Create a data frame for plotting
fdr_df_fash <- data.frame(threshold = threshold_vec, true_fdr = fdr_vec)

# Plot the nominal FDR vs true FDR for FASH

ggplot(fdr_df_fash, aes(x = threshold, y = true_fdr)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "(Nominal) False Discovery Rate", y = "(Actual) False Discovery Rate") +
  theme_minimal() +
  ggtitle("Nominal FDR vs Actual FDR Curve for FASH") +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +
  geom_hline(yintercept = (N - sizeC)/N, linetype = "dashed", color = "red")
```

Also compare the number of false discoveries with the number of discoveries at different level of threshold, where number of discoveries is plotted against number of false discoveries:

```{r}
threshold_vec <- seq(0, 1, by = 0.01)
num_discoveries_vec <- numeric(length(threshold_vec))
num_false_discoveries_vec <- numeric(length(threshold_vec))

for (i in 1:length(threshold_vec)) {
  num_discoveries_vec[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i])
  num_false_discoveries_vec[i] <- sum(fdr_df$cumulative_fdr <= threshold_vec[i] & fdr_df$type != "C")
}
num_discoveries_df <- data.frame(threshold = threshold_vec, num_discoveries = num_discoveries_vec, num_false_discoveries = num_false_discoveries_vec)

ggplot(num_discoveries_df, aes(x = num_discoveries, y = num_false_discoveries)) +
  geom_line() +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "purple") +
  labs(x = "Number of Discoveries", y = "Number of False Discoveries") +
  theme_minimal() +
  # geom_vline(xintercept = sizeC, linetype = "dashed", color = "red") +
  ggtitle("FASH: Number of False Discoveries vs Number of Discoveries") +
  coord_cartesian(xlim = c(0, 1000), ylim = c(0, 1000))
```


Visualize the rank distribution of eQTLs based on the cumulative FDR:
```{r}
ggplot(fdr_df, aes(x = type, y = rank, color = type)) +
  geom_boxplot() +
  labs(x = "Type", y = "Rank") +
  theme_minimal() +
  ggtitle("Rank Distribution: FASH")
```


**Summary:** The FASH method is able to identify many non-linear dynamic eQTLs, with a well-controlled false discovery rate. 
The actual false discovery rate of FASH tends to be more conservative than the nominal false discovery rate, which is expected again as each mixture component of FASH is a L-GP prior that shrinks toward the base model.
